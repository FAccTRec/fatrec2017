
<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FATREC 2017 - Keynote</title>
    <link rel="icon" title="FATREC" href="assets/img/like-q.png">
    <link rel="stylesheet" href="assets/css/app.css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700,700i" rel="stylesheet">
  </head>
  <body>
    <nav class="top-bar">
      <div class="top-bar-title site-title">
        <a href="./">FATREC</a> @ <a href="http://recsys.acm.org/recsys17/">RecSys 2017</a>
      </div>
      <div class="top-bar-right">
        <ul class="menu">
          <li><a href="program.html">Program</a>
          </li><li><a href="cfp.html">Call</a>
          </li><li><a href="committee.html">Committee</a>
          </li><li><a href="https://twitter.com/FAccTRec"><img src="assets/img/twitter.png" style="height: 1em;"></a>
        </li></ul>
      </div>
    </nav>
    <div class="row">
      <div class="large-10 large-offset-1 medium-12 columns end">
        
        <h1>Discrimination in Machine Decision Making</h1>
        <h2>Abstract</h2>
        <p>Machine (data-driven learning-based) decision making is increasingly being used to assist or replace human decision making in a variety of domains ranging from banking (rating user credit) and recruiting (ranking applicants) to judiciary (profiling criminals) and journalism (recommending news-stories). Recently concerns have been raised about the potential for discrimination and unfairness in such machine decisions. Against this background, in this talk, I will pose and attempt to answer the following high-level questions:</p>
        <p>(a) How do machines learn to make discriminatory decision making?
        (b) How can we quantify discrimination in machine decision making?
        (c) How can we control machine discrimination? i.e., can we design learning mechanisms that avoid discriminatory decision making?
        (d) Is there a cost to non-discriminatory decision making?</p>
        <h2>Bio</h2>
        <p>Krishna Gummadi is a tenured faculty member and head of the Networked Systems research group at the Max Planck Institute for Software Systems (MPI-SWS) in Germany. He also holds an honorary professorship at the University of Saarland.</p>
        <p>Krishna's research interests are in the measurement, analysis, design, and evaluation of complex Internet-scale systems. His current projects focus on understanding and building social computing systems. Specifically, they tackle the challenges associated with (i) assessing the credibility of information shared by anonymous online crowds, (ii) understanding and controlling privacy risks for users sharing data on online forums, (iii) understanding, predicting and influencing human behaviors on social media sites (e.g., viral information diffusion), and (iv) enhancing fairness and transparency of machine (data-driven) decision making in social computing systems.</p>
        <p>Krishna's work on online social networks, Internet access networks, and peer-to-peer systems has been widely cited and his papers have received numerous awards, including SIGCOMM Test of Time, IW3C2 WWW Best Paper Honorable Mention, and Best Papers at NIPS ML &amp; Law Symposium, ACM COSN, ACM/Usenix SOUPS, AAAI ICWSM, Usenix OSDI, ACM SIGCOMM IMC, and SPIE MMCN. He has also co-chaired AAAI's ICWSM 2016, IW3C2 WWW 2015, ACM COSN 2014, and ACM IMC 2013 conferences.</p>
        
      </div>
    </div>

    <script src="assets/js/app.js"></script>
  </body>
</html>